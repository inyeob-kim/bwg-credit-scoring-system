import os
import pandas as pd
import numpy as np
from typing import Dict, Any
from fastapi import HTTPException
from app.models.schemas import PredictPayload, ExplainPayload
from app.models.artifacts import load_latest_artifacts
from app.core.utils import prob_to_score
import shap
from dotenv import load_dotenv

load_dotenv()

# ðŸ‘‡ ì¶”ê°€: OpenAI í´ë¼ì´ì–¸íŠ¸
from openai import OpenAI
LLM_MODEL = os.getenv("EXPLAIN_MODEL", "gpt-4o-mini")
_openai_client = None
try:
    _openai_client = OpenAI()  # OPENAI_API_KEYê°€ í™˜ê²½ë³€ìˆ˜ë¡œ ìžˆì–´ì•¼ í•¨
except Exception:
    _openai_client = None


class Predictor:
    def __init__(self, bundle: Dict[str, Any]):
        self.encoder = bundle["encoder"]
        self.selector = bundle["selector"]
        self.model   = bundle["model"]            # Calibrated or Final
        self.meta    = bundle["meta"]
        self.tree_model = bundle.get("tree_model", None)

    @classmethod
    def load_latest(cls):
        bundle = load_latest_artifacts()
        return cls(bundle)

    def _prep(self, records):
        cols = self.meta["num_cols"] + self.meta["cat_cols"]
        df = pd.DataFrame(records)

        # ëˆ„ë½ ì»¬ëŸ¼ ìžë™ ìƒì„±
        for c in cols:
            if c not in df.columns:
                df[c] = np.nan

        df = df[cols].replace({"": np.nan})
        for c in self.meta["num_cols"]:
            df[c] = pd.to_numeric(df[c], errors="coerce")

        X_enc = self.encoder.transform(df)
        X_sel = self.selector.transform(X_enc)
        return X_sel

    def _extract_tree_estimator(self):
        if self.tree_model is not None:
            return self.tree_model

        m = self.model
        if hasattr(m, "calibrated_classifiers_") and m.calibrated_classifiers_:
            cc0 = m.calibrated_classifiers_[0]
            if hasattr(cc0, "estimator"):
                return cc0.estimator
            if hasattr(cc0, "base_estimator"):
                return cc0.base_estimator
        if hasattr(m, "base_estimator_"):
            return m.base_estimator_
        if hasattr(m, "base_estimator"):
            return m.base_estimator
        return None

    # ðŸ‘‡ ì¶”ê°€: ê³ ê° ì¹œí™” ì„¤ëª… ìƒì„±ê¸° (GPT ìš”ì•½)
    def _build_gpt_description(self, record: Dict[str, Any], pd_val: float, score: float, reasons_for_one: list) -> str:
        """
        record: ì› ìž…ë ¥ í•œ ê±´
        pd_val, score: í•´ë‹¹ ê³ ê°ì˜ ì˜ˆì¸¡ ê²°ê³¼
        reasons_for_one: [{"feature": "...", "abs_contrib": ...}, ...]
        """
        if _openai_client is None:
            return "OpenAI client is None"  # LLM ë¯¸ì„¤ì • ì‹œ ì„¤ëª… ìƒëžµ(ì¡°ìš©ížˆ ì‹¤íŒ¨)

        print("[DEBUG] API KEY =", os.getenv("OPENAI_API_KEY"))

        # ì½ê¸° ì‰¬ìš´ í‚¤ ë¼ë²¨(ì›í•˜ë©´ í™•ìž¥)
        label_map = {
            "utilization": "ì‹ ìš©í•œë„ ì‚¬ìš©ë¥ ",
            "credit_utilization": "ì‹ ìš©í•œë„ ì‚¬ìš©ë¥ (ì›)",
            "debt_ratio": "ë¶€ì±„ë¹„ìœ¨",
            "credit_history_months": "ì‹ ìš©ì´ë ¥ ê°œì›”",
            "n_late_30d": "30ì¼ ì´ìƒ ì—°ì²´ íšŸìˆ˜",
            "income": "ì†Œë“",
            "loan_amount": "ëŒ€ì¶œê¸ˆì•¡",
            "num_of_loans": "ëŒ€ì¶œê±´ìˆ˜",
            "employment_length": "í˜„ ì§ìž¥ ê·¼ì†ì—°ìˆ˜",
            "employment_type": "ê³ ìš©í˜•íƒœ",
            "region": "ì§€ì—­",
            "gender": "ì„±ë³„",
            "channel": "ì±„ë„",
        }

        # ìƒìœ„ ìš”ì¸ í…ìŠ¤íŠ¸ êµ¬ì„± (ë°©í–¥ ì •ë³´ê°€ ì—†ë‹¤ë©´ ì˜í–¥ë„ê°€ í° ìˆœì„œë¡œë§Œ ìš”ì•½)
        reasons_lines = []
        if reasons_for_one:
            for r in reasons_for_one:
                feat = r.get("feature", "")
                nm = label_map.get(feat, feat)
                reasons_lines.append(f"- {nm} (ì˜í–¥ë„: {r.get('abs_contrib', 0):.3f})")
        reasons_text = "\n".join(reasons_lines) if reasons_lines else "- ìƒìœ„ ì˜í–¥ ìš”ì¸ ì •ë³´ ì—†ìŒ"

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = f"""
                ë„ˆëŠ” ê¸ˆìœµ ìƒë‹´ì‚¬ë‹¤. ì•„ëž˜ ê³ ê°ì˜ ì‹ ìš©í‰ê°€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¼ë°˜ ê³ ê°ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ 3~5ì¤„ë¡œ ìš”ì•½í•´ë¼.
                ì „ë¬¸ìš©ì–´ëŠ” í’€ì–´ì„œ ì“°ê³ , ì ìˆ˜/ë¶€ë„í™•ë¥ ì˜ ì˜ë¯¸ë¥¼ í•¨ê»˜ ì„¤ëª…í•´ë¼.

                [ê³ ê° ìž…ë ¥ ìš”ì•½]
                { {k: record.get(k) for k in list(record.keys())[:12]} }  # ì£¼ìš” í•­ëª©ë§Œ

                [ì˜ˆì¸¡ ê²°ê³¼]
                - ì˜ˆìƒ ë¶€ë„í™•ë¥ (PD): {pd_val:.5f}
                - ì‹ ìš©ì ìˆ˜(Score): {score:.0f}

                [ìƒìœ„ ì˜í–¥ ìš”ì¸]
                {reasons_text}

                ì„¤ëª… ì‹œ 'ì™œ ì´ë ‡ê²Œ ë‚˜ì™”ëŠ”ì§€'ë¥¼ í•µì‹¬ ìš”ì¸ ì¤‘ì‹¬ìœ¼ë¡œ í’€ì–´ì„œ ë§í•˜ê³ ,
                ê°œì„  ì—¬ì§€ê°€ ìžˆëŠ” í•­ëª©(ì˜ˆ: í•œë„ ì‚¬ìš©ë¥ , ë¶€ì±„ë¹„ìœ¨)ì´ ë†’ìœ¼ë©´ ê´€ë¦¬ íŒ í•œ ì¤„ì„ ë§ë¶™ì—¬ë¼.
                """

        try:
            resp = _openai_client.chat.completions.create(
                model=LLM_MODEL,
                temperature=0.2,
                max_tokens=220,
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ì¹œì ˆí•œ ê¸ˆìœµ ìƒë‹´ì‚¬ë‹¤. ê³ ê° ëˆˆë†’ì´ì— ë§žê²Œ ê°„ê²°í•˜ê³  ì •í™•ížˆ ì„¤ëª…í•´ë¼."},
                    {"role": "user", "content": user_prompt}
                ],
            )
            return resp.choices[0].message.content.strip()
        except Exception:
            return ""  # ì‹¤íŒ¨ ì‹œ ì¡°ìš©ížˆ íŒ¨ìŠ¤

    def predict(self, payload: PredictPayload) -> Dict[str, Any]:
        X_sel = self._prep(payload.records)

        use_raw = bool(getattr(payload, "use_raw", False))
        if use_raw:
            base_est = self._extract_tree_estimator()
            if base_est is None:
                raise HTTPException(status_code=500, detail="Underlying tree estimator not found for raw proba.")
            pd_hat = base_est.predict_proba(X_sel)[:, 1]
        else:
            pd_hat = self.model.predict_proba(X_sel)[:, 1]

        # ìˆ˜ì¹˜ ì•ˆì •ì„±: 0/1 í™•ë¥  ë°©ì§€
        pd_hat = np.clip(pd_hat, 1e-5, 1 - 1e-5)

        scores = prob_to_score(pd_hat)

        # ê¸°ë³¸ ê²°ê³¼ í‹€
        result: Dict[str, Any] = {
            "version": self.meta["version"],
            "n": len(payload.records),
            "predictions": [],
        }

        # reasonsë¥¼ ë¨¼ì € ê³„ì‚° (ì„¤ëª…ì— í™œìš©)
        reasons_batch = None
        if payload.return_reasons:
            try:
                base_est = self._extract_tree_estimator()
                if base_est is None:
                    raise RuntimeError("Underlying tree estimator not found for SHAP.")
                explainer = shap.TreeExplainer(base_est)
                shap_vals = explainer.shap_values(X_sel)
                if isinstance(shap_vals, list):
                    class_idx = 1 if len(shap_vals) > 1 else 0
                    shap_arr = shap_vals[class_idx]
                else:
                    shap_arr = shap_vals

                feat_names = self.meta["selected_features"]
                if len(feat_names) != X_sel.shape[1]:
                    raise RuntimeError(f"Selected feature count mismatch: meta={len(feat_names)} vs X={X_sel.shape[1]}")
                topn = max(1, min(int(payload.top_n_reasons), len(feat_names)))

                reasons_batch = []
                for i in range(min(len(payload.records), 200)):
                    sv = np.abs(shap_arr[i])
                    top_idx = np.argsort(-sv)[:topn]
                    reasons_one = [{"feature": feat_names[j], "abs_contrib": float(sv[j])} for j in top_idx]
                    reasons_batch.append(reasons_one)
            except Exception as e:
                result["reasons_error"] = str(e)
                reasons_batch = [None] * len(payload.records)

        # ê° ë ˆì½”ë“œë³„ description ìƒì„± + predictions ì±„ìš°ê¸°
        for i, (p, s) in enumerate(zip(pd_hat, scores)):
            reasons_for_one = reasons_batch[i] if reasons_batch else None
            desc = self._build_gpt_description(
                record=payload.records[i],
                pd_val=float(p),
                score=float(s),
                reasons_for_one=reasons_for_one or []
            )
            item = {
                "pd": float(p),
                "score": float(s),
                "description": desc  # ðŸ‘ˆ ê³ ê°ìš© ì„¤ëª… ì¶”ê°€
            }
            # ìš”ì²­ì´ ìžˆìœ¼ë©´ reasonsë„ ê°™ì´ í¬í•¨
            if reasons_for_one is not None:
                item["reasons"] = reasons_for_one
            result["predictions"].append(item)

        print("Result = ", result)
        return result

    def explain(self, payload: ExplainPayload) -> Dict[str, Any]:
        X_sel = self._prep(payload.records)
        base_est = self._extract_tree_estimator()
        if base_est is None:
            raise HTTPException(status_code=500, detail="Underlying tree estimator not found for SHAP.")
        explainer = shap.TreeExplainer(base_est)
        maxn = min(payload.max_records, X_sel.shape[0])
        shap_vals = explainer.shap_values(X_sel[:maxn])
        if isinstance(shap_vals, list):
            class_idx = 1 if len(shap_vals) > 1 else 0
            shap_arr = shap_vals[class_idx]
        else:
            shap_arr = shap_vals
        feat_names = self.meta["selected_features"]
        if len(feat_names) != X_sel.shape[1]:
            raise HTTPException(status_code=500, detail=f"Selected feature count mismatch: meta={len(feat_names)} vs X={X_sel.shape[1]}")
        abs_mean = np.mean(np.abs(shap_arr), axis=0)
        global_imp = [{"feature": f, "abs_mean_contrib": float(v)}
                      for f, v in sorted(zip(feat_names, abs_mean), key=lambda x: -x[1])][:50]
        return {"version": self.meta["version"], "n_used": int(maxn), "global_importance": global_imp}
